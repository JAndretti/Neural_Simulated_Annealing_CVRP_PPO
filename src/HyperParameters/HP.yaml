# =============== Project & Logging ===============
PROJECT: "Neural_Simulated_Annealing"  # Project name
GROUP: "ATTN"              # Experimentation group
LOG: True                             # Enable or disable logging with WandB
VERBOSE: False                          # Verbose output during training

# =============== Hardware & Setup ===============
DEVICE: "cuda"                          # Hardware used: cpu, cuda, or mps
SEED: 42                               # Random seed for reproducibility

# =============== CVRP Problem ===============
PROBLEM_DIM: 50                       # Number of nodes in the CVRP problem
MAX_LOAD: 40                           # Maximum vehicle capacity
N_PROBLEMS: 256                        # Number of parallel instances
DATA: "uchoa"                          # Type of data: random or uchoa

# =============== Heuristic & Initial Solution ===============
HEURISTIC: ["insertion"]                 # Local search method (insertion, swap, two_opt)

UPDATE_METHOD: "free"                  # Update method after heuristic
INIT: "random"                        # Initial solution method
MULTI_INIT: True                       # Use multiple initial solutions
INIT_LIST: ["random", "sweep", "Clark_and_Wright"] # List of initial solution methods

# =============== Reward ===============
CAPACITY_REWARD: False                  # Reward for capacity usage
CAPACITY_REWARD_FACTOR: 10             # Scaling factor for capacity reward
REWARD: "immediate"                    # Reward calculation method
REWARD_LAST: False
REWARD_LAST_ADD: 0.001
REWARD_VALID: False                   # penalize invalid solutions in reward
NORMALIZE_REWARD: False               # Normalize reward to have mean 0 and std 1

# =============== Model Architecture ===============
MODEL: "attention"                   # Model type: pairs, seq, attention
ENTRY: 17                              # Input dimension
EMBEDDING_DIM: 128                      # Embedding dimension
NUM_H_LAYERS: 1                        # Number of layers in the model

# =============== Training ===============
N_EPOCHS: 10000                         # Total number of training epochs
LR_ACTOR: 0.001                              # Learning rate
LR_CRITIC: 0.001                              # Learning rate
WEIGHT_DECAY: 0.001                    # L2 regularization
BATCH_SIZE: 1024                        # Training batch size 

# =============== Simulated Annealing ===============
INIT_TEMP: 1                           # Initial temperature
STOP_TEMP: 0.1                         # Final temperature (to low tend to bad learning bcs rejection)
OUTER_STEPS: 100                       # Number of temperature steps
INNER_STEPS: 1                         # Number of steps per temperature
SCHEDULER: "lam"                      # Cooling schedule type
METROPOLIS: True                       # Use Metropolis criterion

# =============== PPO Algorithm ===============
METHOD: "ppo"                          # Training algorithm used
PPO_EPOCHS: 10                         # Number of PPO optimization steps
TRACE_DECAY: 0.95                      # Lambda parameter for GAE
EPS_CLIP: 0.25                         # Policy clipping range
LOSS: "min_cost"                       # Optimization objective
GAMMA: 0.99                            # Discount factor
ENT_COEF: 0.01                         # Entropy coefficient
TARGET_KL: 0.03                        # Target KL divergence
BETA_KL: 1.0                           # KL penalty coefficient
GP_LAMBDA: 0.01                        # Gradient penalty weight

# =============== Testing during training ===============
TEST_DIMENSION: 50                    # Test dimension (50 or 100)
TEST_NB_PROBLEMS: 1000                  # Number of test problems (less than 10,000 for faster testing)
TEST_OUTER_STEPS: 100
NAZARI: True                          # Use Nazari et al. (2018) initial solution for testing
TEST_INIT: "random"
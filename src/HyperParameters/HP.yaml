# =============== Project & Logging ===============
PROJECT: "Neural_Simulated_Annealing"  # Project name
GROUP: "ARCH"              # Experimentation group
LOG: False                             # Enable or disable logging with WandB
VERBOSE: False                          # Verbose output during training

# =============== Hardware & Setup ===============
DEVICE: "cpu"                          # Hardware used: cpu, cuda, or mps
SEED: 42                               # Random seed for reproducibility

# =============== CVRP Problem ===============
PROBLEM_DIM: 50                       # Number of nodes in the CVRP problem
MAX_LOAD: 40                           # Maximum vehicle capacity
N_PROBLEMS: 256                        # Number of parallel instances

# ============== Clustering ===============
CLUSTERING: False                      # Enable or disable clustering
NB_CLUSTERS_MAX: 10                    # Maximum number of clusters if clustering is enabled
ALT_CLUSTERING: False                   # Use alternate clustering during training

# =============== Heuristic & Initial Solution ===============
HEURISTIC: "two_opt"                 # Local search method
MIX1: "swap"                           # First local search method if HEURISTIC is "mix"
MIX2: "insertion"                      # Second local search method if HEURISTIC is "mix"

INIT: "isolate"                        # Initial solution method
UPDATE_METHOD: "rm_depot"                  # Update method after heuristic
CHANGE_INIT_METHOD: False               # Change initial method during training
INIT_LIST: ["random","isolate","sweep","Clark_and_Wright","nearest_neighbor", "cheapest_insertion", "path_cheapest_arc"]                          # List of initial solution methods

# =============== Reward ===============
NORMALIZE_REWARD: False                # Normalize rewards
CAPACITY_REWARD: False                  # Reward for capacity usage
CAPACITY_REWARD_FACTOR: 10             # Scaling factor for capacity reward
NEG_REWARD: 0                          # Negative reward for invalid solutions
REWARD: "immediate"                    # Reward calculation method

# =============== Model Architecture ===============
ENTRY: 15                              # Input dimension
EMBEDDING_DIM: 32                      # Embedding dimension
NUM_H_LAYERS: 1                        # Number of layers in the model
PAIRS: False                            # Enable or disable pair-based model

# =============== Training ===============
N_EPOCHS: 1000                         # Total number of training epochs
LR: 0.001                              # Learning rate
WEIGHT_DECAY: 0.001                    # L2 regularization
BATCH_SIZE: 512                        # Training batch size

# =============== Simulated Annealing ===============
INIT_TEMP: 1                           # Initial temperature
STOP_TEMP: 0.1                         # Final temperature
OUTER_STEPS: 200                       # Number of temperature steps
INNER_STEPS: 1                         # Number of steps per temperature
SCHEDULER: "lam"                      # Cooling schedule type
METROPOLIS: True                       # Use Metropolis criterion

# =============== PPO Algorithm ===============
METHOD: "ppo"                          # Training algorithm used
PPO_EPOCHS: 10                         # Number of PPO optimization steps
TRACE_DECAY: 0.95                      # Lambda parameter for GAE
EPS_CLIP: 0.25                         # Policy clipping range
LOSS: "min_cost"                       # Optimization objective
GAMMA: 0.99                            # Discount factor
ENT_COEF: 0.01                         # Entropy coefficient
TARGET_KL: 0.01                        # Target KL divergence
GP_LAMBDA: 0.01                        # Gradient penalty weight

# =============== Testing during training ===============
TEST_DIMENSION: 50                    # Test dimension (50 or 100)
TEST_NB_PROBLEMS: 5000                  # Number of test problems (less than 10,000 for faster testing)
